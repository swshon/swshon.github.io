<!doctype HTML>
<html>

<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>

    <title>Suwon Shon</title>

    <link rel="stylesheet" href="stylesheet.css?ver=2"> 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-68509416-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-68509416-2');
</script>


   <meta charset="UTF-8"> 
  </head>
<body alink="#ff0000" bgcolor="#ffffff" text="#000000" vlink="#760076" link="#0000a3">
<center>
<a name="top"></a>

<table width="650">
<tbody><tr>
<td>
<br>
<table width=720><tr><td align=center>

</td>
</tr>
</table>
<br>
    
<div id="shadow" style="padding-right: 10px;">
<!--<div id="shadow" style="background-image: url(img/main_logo.png); background-repeat: repeat-x; background-size: 89px;">-->
<table cellpadding="10" width=750>
<tr>
<td>
<table cellpadding = "1" width=450>
<tr><td align="top"><left><h1><i>Suwon Shon</i></h1></left></td></tr>
<tr><td align="top"><left><h2>32 Vassar St., 32-G436<br>Cambridge, MA, USA <br><br> swshon (at) csail (dot) mit (dot) edu </h2>
    <br><a href="pdf/resume_suwon.pdf" target="_blank">Resume</a> / <a href="https://www.csail.mit.edu/person/suwon-shon" target="_blank">CSAIL Profile</a> / <a href="https://scholar.google.com/citations?user=-SfNafoAAAAJ&hl=en" target="_target">Google Scholar</a></left></td></tr>
    
</table>
</td>
<td>
    <img src="img/shon.jpg">
    </td>
</tr>
</table>



    <p> I'm a research scientist at <a href="http://groups.csail.mit.edu/sls" target="_blank">Spoken Language Systems group</a> of MIT <a href="http://csail.mit.edu" target="_blank">Computer Science and Artificial Intelligence Laboratory (CSAIL)</a> working with Dr. <a href="http://groups.csail.mit.edu/sls/people/glass.shtml" target="_blank">James Glass</a>. I received B.S and integrated Ph.D. degree in electrical engineering from Korea University at South Korea in 2010 and 2017, respectively. I joined MIT as postdoctoral associate in 2017. My research focuses on machine learning technologies for speech signal processing and I have been working on speaker and language recognition and related pre-processing techniques.
    </p>


<a name="purpose"></a>
<!--     <h3><a href="pdf/CV_suwon.pdf" target="_blank">Curriculum Vitae</a></h3>
 -->
<h3>Recent work and projects</h3>
    <ul>
	<li>(Apr.2019) Organizing Arabic Dialect Identification (ADI) track of the Fifth edition of the Multi-
Genre Broadcast Challenge (MGB-5) [<a href="https://arabicspeech.org/mgb5" target="_blank">website</a>] [<a href="https://github.com/swshon/arabic-dialect-identification" target="_blank">baseline</a>] [<a href="http://groups.csail.mit.edu/sls/downloads/adi17/" target="_blank">ADI17 dataset</a>]</li>
<ul>
<li>We aim to present fine-grained analysis of Arabic dialects.
<li>Over 3,000 hours of Arabic dialect speech data of 17 Arabic countries collected from the YouTube.
<li>A challenge special session will be held in <a href="http://www.asru2019.org/wp/?page_id=622" target="_blank">ASRU 2019</a>.
</ul>
        <li><i>(May~Oct. 2018)</i> Participated NIST Speaker Recognition Evaluation (SRE) 2018 as member of JHU-MIT Team [<a href="http://groups.csail.mit.edu/sls/publications/2018/Shon_NIST-2018.pdf" target="_blank">system description</a>]</li>
        <li><i>(May 2018)</i> Organizing <a href="http://mce.csail.mit.edu" target="_blank">MCE 2018</a> [<a href="http://mce.csail.mit.edu/" target="_blank">website</a>] [<a href="http://mce.csail.mit.edu/mce2018-plan_v3.pdf" target="_blank">plan</a>] [<a href="https://github.com/swshon/multi-speakerID" target="_blank">code</a>] [<a href="https://www.kaggle.com/kagglesre/blacklist-speakers-dataset" target="_blank">dataset</a>]</li>
        <li><i>(Mar. 2018)</i> Organizing task in <a href="http://alt.qcri.org/vardial2018/" target="_blank">Vardial 2018</a> [<a href="https://github.com/swshon/dialectID_e2e/" target="_blank">code</a>]</li>
        <ul>
            <li>If you want to start the Arabic Dialect Identification task with dialect embeddings, you can download <a href="https://github.com/swshon/dialectID_e2e/tree/master/vardial2018" target="_blank">here</a></li>
            <li>The complete program and papers are available at workshop of COLING 2018 [<a href="http://web.science.mq.edu.au/~smalmasi/vardial5/" target="_blank">link</a>]</li>
        </ul>
        <li><i>(Feb. 2018)</i> Real-time Arabic dialect identification is online! you can find the system <a href="https://dialectid.qcri.org/" target="_blank">here</a><br>
            This system had <a href="https://2018.ieeeicassp.org/Demos.asp" target="_blank">demo session</a> on ICASSP 2018 [<a href="pdf/adi_system_2018.pdf" target="_blank">slide</a>]<br>
            Detailed system architecture can be found on <a href="https://github.com/swshon/dialectID_e2e" target="_blank">github repo</a>.</li>
        <li><i>(Dec. 2017)</i> I lead MIT-QCRI team for the 3rd <a href="http://www.mgb-challenge.org/" target="_blank">Multi-Genre Broadcast (MGB-3) Challenge </a> on Arabic Dialect Identification (ADI) task and <i><b>we won the challenge!</b></i> <ul><li> MIT-QCRI team <a href="https://arxiv.org/abs/1709.00387" target="_blank">paper</a> was presented in ASRU 2017</li><li>We marked 75% overall accuracy which is significantly higher than second team (70%) and detail results can be found on the <a href="https://arxiv.org/pdf/1709.07276.pdf" target="_blank">summary paper</a>.</li><li> We further improved performance of the system and now it is 81% on MGB-3 Test set which is best accuracy reported until now. Check paper [14] below. (Feb.28 2018)</li></ul> </li> 


        
    </ul>    

<h3>Presentations</h3>
    <ul>
	<li>"Overview of Automatic Speaker Recognition", Network course at MIT Beaverworks, MIT Lincoln Lab., Cambridge, MA, USA, Aug. 16, 2019</li>
<!--	<li>"Deep learning models for voice identity", Amazon, Sunnyvale, CA, USA, Jul. 24, 2019</li>
	<ul>
	<li>Same topic was presented at Apple, Cupertino, CA, USA (Jul. 26, 2019), ASAPP, New York City, NY, USA (Aug. 1, 2019), Facebook, Menlo Park, CA, USA (Aug. 26, 2019)
	</ul>-->
        <li>"Robust Speaker Recognition influenced by noise and face", ETRI, Daejeon, South Korea, Apr. 29, 2019</li>
	<ul>
	<li>Same topic was presented at Naver (Apr. 24, 2019), KAIST (Apr. 17, 2019) and Korea university (Apr. 16, 2019).</li>
	</ul>
        <li>"Analyzing hidden representation of end-to-end speaker recognition system", KAIST, Daejeon, South Korea, Jul. 5, 2018</li>
	<ul>
		<li>Same topic was presented at Kookmin University (Jul. 7, 2018), Korea University (Jul. 7, 2018), NCSOFT (Jul. 6, 2018) and Naver (Jul. 11, 2018).</li>

	</ul>
        <li>"Recent Speaker Recognition progress", Philips Visit Day, Cambridge, MA, USA, Apr. 11 2018 </li>
        <li>"Speaker / Dialect Recognition under Limited Resources", Qatar Computing Research Institute, Doha, Qatar, Nov. 14, 2017 </li>
        <li>“Autoencoder based Domain Adaptation for Speaker Recognition under Insufficient Channel Information”, Interspeech 2017, Stockholm, Sweden, Aug. 22, 2017</li>
        
    </ul>    



<h3>Publications (Peer-reviewed)</h3>

    <table><tr><td align=left>
        <table cellpadding="5">                     

            <tbody><tr><td align="right" valign="top">[-]</td><td> Ahmed Ali, <b>Suwon Shon</b>, Younes Samih, Hamdy Mubarak, Ahmed Abdelali, James Glass, Steve Renals, Khalid Choukri, “The MGB-5 Challenge: Recognition and Dialect Identification of Dialectal Arabic Speech”, <i>to appear in ASRU</i>, pp. -, Singapore, December 2019 [<a href="http://www.mgb-challenge.org/MGB-5.html" target="_blank">website</a>]</td></tr></tbody>

            <tbody><tr><td align="right" valign="top">[25]</td><td><b>Suwon Shon</b>, Hao Tang, James Glass, “VoiceID Loss: Speech Enhancement for Speaker Verification”, <i>Interspeech</i>, pp. 2888-2892, Graz, Austria, September 2019 [<a href="pdf/shon_interspeech2019_voiceid.pdf" target="_blank">pdf</a>][<a href="https://arxiv.org/abs/1904.03601" target="_blank">arxiv</a>][<a href="https://people.csail.mit.edu/swshon/supplement/voiceid-loss/" target="_blank">demo</a>]</td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[24]</td><td><b>Suwon Shon</b>, Younggun Lee, Taesu Kim, “Large-scale Speaker Retrieval on Random Speaker Variability Subspace”, <i>Interspeech</i>, pp. 2963-2967, Graz, Austria, September 2019 [<a href="pdf/shon_interspeech2019_large.pdf" target="_blank">pdf</a>][<a href="https://arxiv.org/abs/1811.10812" target="_blank">arxiv</a>]</td></tr></tbody>

            <tbody><tr><td align="right" valign="top">[23]</td><td><b>Suwon Shon</b>, Najim Dehak, Douglas Reynolds, James Glass, “MCE 2018: The 1st Multi-target Speaker Detection and Identification Challenge Evaluation”, <i>Interspeech</i>, pp. 356-360, Graz, Austria, September 2019 [<a href="pdf/shon_interspeech2019_mce.pdf" target="_blank">pdf</a>][<a href="https://arxiv.org/abs/1904.04240" target="_blank">arxiv</a>]</td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[22]</td><td>Jesús Villalba, Nanxin Chen, David Snyder, Daniel Garcia-Romero, Alan McCree, Gregory Sell, Jonas Borgstrom, Fred Richardson, <b>Suwon Shon</b>, François Grondin, Réda Dehak, Leibny Paola García-Perera, Daniel Povey, Pedro A. Torres-Carrasquillo, Sanjeev Khudanpur, Najim Dehak, “State-of-the-art Speaker Recognition for Telephone and Video Speech: the JHU-MIT Submission for NIST SRE18”, <i>Interspeech</i>, pp. 1488-1492, Graz, Austria, September 2019 [<a href="pdf/villalba_interspeech2019_state.pdf" target="_blank">pdf</a>]</td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[21]</td><td>Achintya K. Sarkar, Zheng-Hua Tan, Hao Tang, <b>Suwon Shon</b> and James Glass, "Time-Contrastive Learning Based Deep Bottleneck Features for Text-Dependent Speaker Verification", <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing</i>, vol. 27, no. 8, pp. 1267-1279, August 2019 [<a href="https://arxiv.org/abs/1905.04554" target="_blank">arxiv</a>]</td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[20]</td><td><b>Suwon Shon</b>, Tae-Hyun Oh, James Glass, “Noise-tolerant Audio-Visual Online Person Verification using an Attention-based Neural Network Fusion”, <i>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, pp. 3995-3999, Brighton, UK, May 2019 [<a href="./pdf/shon_icassp2019_noise.pdf" target="_blank">pdf</a>][<a href="https://arxiv.org/abs/1811.10813" target="_blank">arxiv</a>][<a href="pdf/shon_icassp2019_noise_poster.png" target="_blank">poster</a>]</td></tr></tbody>

            <tbody><tr><td align="right" valign="top">[19]</td><td><b>Suwon Shon</b>, Ahmed Ali, James Glass, “Domain Attentive Fusion for End-to-end Dialect Identification with Unknown Target Domain”, <i>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, pp. 5951-5955, Brighton, UK, May 2019 [<a href="./pdf/shon_icassp2019_domain.pdf" target="_blank">pdf</a>][<a href="https://arxiv.org/abs/1812.01501" target="_blank">arxiv</a>][<a href="pdf/shon_icassp2019_domain_poster.pdf" target="_blank">poster</a>]</td></tr></tbody>

            <tbody><tr><td align="right" valign="top">[18]</td><td>Seongkyu Mun, <b>Suwon Shon</b>, “Domain Mismatch Robust Acoustic Scene Classification using Channel Information Conversion”, <i>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, pp. 845-849, Brighton, UK, May 2019 [<a href="./pdf/mun_icassp2019_domain.pdf" target="_blank">pdf</a>][<a href="https://arxiv.org/abs/1812.01731" target="_blank">arxiv</a>] </td></tr></tbody>

            <tbody><tr><td align="right" valign="top">[17]</td><td><b>Suwon Shon</b>, Wei-Ning Hsu and James Glass, “Unsupervised Representation Learning of Speech for Dialect Identification”, <i>IEEE Workshop on Spoken Language Technology (SLT)</i>, pp. 105-111, Athens, Greece, Dec. 2018, [<a href="./pdf/shon_slt18_did.pdf" target="_blank">pdf</a>][<a href="https://arxiv.org/abs/1809.04458" target="_blank">arxiv</a>][<a href="pdf/slt2018_unsuper_poster.pdf" target="_blank">poster</a>]</td></tr></tbody>

            <tbody><tr><td align="right" valign="top">[16]</td><td><b>Suwon Shon</b>, Hao Tang and James Glass, “Frame-level Speaker Embeddings for Text-independent Speaker Recognition and Analysis of End-to-end Model”, <i>IEEE Workshop on Spoken Language Technology (SLT)</i>, pp. 1007-1013, Athens, Greece, Dec. 2018, [<a href="./pdf/shon_slt18_sid.pdf" target="_blank">pdf</a>][<a href="https://arxiv.org/abs/1809.04437" target="_blank">arxiv</a>][<a href="pdf/slt2018_frame_poster.pdf" target="_blank">poster</a>][<a href="https://people.csail.mit.edu/swshon/supplement/slt18.html" target="_blank">Supplementary figures</a>]</td></tr></tbody>

            <tbody><tr><td align="right" valign="top">[15]</td><td>Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Ahmed Ali, <b>Suwon Shon</b>, James Glass and others, “Language Identification and Morphosyntactic Tagging:
The Second VarDial Evaluation Campaign”, <i>The Fifth Workshop on NLP for Similar
Languages, Varieties and Dialects (VarDial) of COLING </i>, pp. 1-17, Santa Fe, USA, Aug. 2018 [<a href="./pdf/shon_2018_vardial.pdf" target="_blank">pdf</a>]
            
            <tbody><tr><td align="right" valign="top">[14]</td><td><b>Suwon Shon</b>, Ahmed Ali and James Glass, “Convolutional Neural  Networks and Language Embeddings for End-to-End Dialect Recognition”, <i>Speaker Odyssey: The Speaker and Language Recognition Workshop</i>, pp. 98-104, Les Sables d'Olonne, France, June 2018 [<a href="https://arxiv.org/abs/1803.04567" target="_blank">pdf</a>][<a href="pdf/odyssey_2018.pdf" target="_blank">poster</a>][<a href="https://github.com/swshon/dialectID_e2e" target="_blank">code</a>]</td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[13]</td><td>Maryam Najafian, Sameer Khurana, <b>Suwon Shon</b>, Ahmed Ali and James Glass, “Exploiting Convolutional Neural Network for Phonotactic based Dialect Identification”, <i>IEEE International Conference on Acoustics, Speechand Signal Processing (ICASSP)</i>, pp. 5174-5178, Calgary, Canada, April 2018 [<a href="pdf/maryam_2018.pdf" target="_blank">pdf</a>] [<a href="pdf/icassp_2018.pdf" target="_blank">poster</a>]</td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[12]</td><td><b>Suwon Shon</b>, Ahmed Ali and James Glass, “MIT-QCRI Arabic Dialect Identification System for the 2017 Multi-Genre Broadcast Challenge”, <i>IEEE Automatic Speech Recognition and Understanding (ASRU) Workshop</i>, pp. 374-380, Okinawa, Japan, December 2017 [<a href="https://arxiv.org/abs/1709.00387" target="_blank">pdf</a>] [<a href="pdf/asru_2017.pdf" target="_blank">poster</a>] [<a href="https://github.com/swshon/dialectID_siam" target="_blank">code</a>]</td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[11]</td><td><b>Suwon Shon</b>, Seongkyu Mun, Wooil Kim and Hanseok Ko, “Autoencoder based Domain Adaptation for Speaker Recognition under Insufficient Channel Information”, <i>Interspeech</i>, pp. 1014-1018, Stockholm, Sweden, August 2017 [<a href="https://arxiv.org/abs/1708.01227" target="_blank">pdf</a>] [<a href="pdf/interspeech2017_aeda.pdf" target="_blank">slide</a>] </td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[10]</td><td><b>Suwon Shon</b>, Seongkyu Mun and Hanseok Ko, “Recursive Whitening Transformation for Speaker Recognition on Language Mismatched Condition”, <i>Interspeech</i>, pp. 2869-2873, Stockholm, Sweden, August 2017 [<a href="https://arxiv.org/abs/1708.01232" target="_blank">pdf</a>] [<a href="pdf/interspeech2017_whitening_vertical.pdf"  target="_blank">poster</a>] </td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[9]</td><td>Seongkyu Mun, <b>Suwon Shon</b>, Wooil Kim, David Han and Hanseok Ko, “Deep Neural Network based Learning and Transferring Mid-level Auto Features for Acoustic Scene Classification”, <i>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, pp. 796-800, New Orleans, USA, March 2017 [<a href="pdf/skmun_2017.pdf" target="_blank">pdf</a>] </td></tr></tbody>
                        
            <tbody><tr><td align="right" valign="top">[8]</td><td>Seongkyu Mun, <b>Suwon Shon</b>, Wooil Kim and Hanseok Ko,  “Deep  Neural  Network  Bottleneck  Features  for Acoustic Event Recognition”, <i>Interspeech</i>, pp. 2954-2957, San Francisco, CA, USA, September 2016 [<a href="pdf/skmun_2016.pdf" target="_blank">pdf</a>]</td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[7]</td><td><b>Suwon Shon</b>, Seongkyu Mun, David Han and Hanseok Ko, “A non-negative matrix factorization based subband decomposition for acoustic source localization”, <i>Electronics Letters</i>, Vol.51, No.22, pp. 1723-1724, 2015 [<a href="https://arxiv.org/abs/1610.04695" target="_blank">pdf</a>]</td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[6]</td><td><b>Suwon Shon</b>, Seungkyu Mun, David Han, Hanseok Ko, “Maximum Likelihood Linear Dimension Reduction of Heteroscedastic Feature for Robust Speaker Recognition”, <i>IEEE International Conference on Advanced Video and Signal-based Surveillance (AVSS)</i>, Karlsruhe, Germany, August 25-28, 2015 [<a href="pdf/shon_2015.pdf" target="_blank">pdf</a>]</td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[5]</td><td>Seungkyu Mun, <b>Suwon Shon</b>, Wooil Kim, Hanseok Ko, “Generalized cross-correlation based noise robust abnormal acoustic event localization utilizing non-negative matrix factorization”, <i>IEEE International Conference on Advanced Video and Signal-based Surveillance (AVSS)</i>, Seoul, South Korea, September 26-29, 2014 [<a href="pdf/skmun_2014_avss.pdf" target="_blank">pdf</a>]</td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[4]</td><td><b>Suwon Shon</b>, David K. Han, and Hanseok Ko, “Abnormal Acoustic Event Localization based on Selective Frequency  Bin  in  High  Noise  Environment  for  Audio  Surveillance”, <i>IEEE International Conference on AdvancedVideo and Signal-based Surveillance (AVSS)</i>, pp. 87-92 , Krakow, Poland, August 2013 [<a href="pdf/shon_2013.pdf" target="_blank">pdf</a>]</td></tr></tbody>

            <tbody><tr><td align="right" valign="top">[3]</td><td><b>Suwon Shon</b>, David K. Han, Jounghoon Beh and Hanseok Ko, “Full Azimuth Multiple Sound Source localization with 3-channel microphone array”, <i>IEICE Trans. on Fundamentals</i>, Vol. E95-A, No. 4 pp. 745-750, April 2012 [<a href="pdf/shon_2012_localization.pdf" target="_blank">pdf</a>]</td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[2]</td><td><b>Suwon Shon</b>, Eric Kim, Jongsung Yoon and Hanseok Ko, “Sudden Noise Source Localization System for intelligent Automobile application with Acoustic Sensors”, <i>IEEE International Conference on Consumer Electronics</i>, pp. 237-238, Las Vegas, NV, USA, January 2012 [<a href="pdf/shon_2012.pdf" target="_blank">pdf</a>]</td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[1]</td><td><b>Suwon Shon</b>, Jounghoon Beh, Cheoljong Yang, David K. Han and Hanseok Ko, “Motion Primitives for Designing Flexible Gesture Set in Human-Robot Interface”, <i>International Conference on Control, Automation and Systems</i>, pp. 1501-1504, Il-san, South Korea, October 2011 [<a href="pdf/shon_2011.pdf" target="_blank">pdf</a>]</td></tr></tbody>
            
            
        </table>
        </td>
        </tr>
    </table>
    

    
    
    


<h3>Manuscripts (Non-peer-reviewed)</h3>

    <table><tr><td align=left>
        <table cellpadding="5">
            
            <tbody><tr><td align="right" valign="top">[4]</td><td>Younggun Lee, <b>Suwon Shon</b>, Taesu Kim, “Learning pronunciation from a foreign language in speech synthesis networks”, in preparation, [<a href="https://arxiv.org/abs/1811.09364" target="_blank">arxiv</a>] </td></tr></tbody>

            <tbody><tr><td align="right" valign="top">[3]</td><td>Jesus Villalba, Nanxin Chen, David Snyder, Daniel Garcia-Romero, Alan McCree, Gregory Sell, Jonas Borgstrom, Fred Richardson, <b>Suwon Shon</b>, François Grondin, Reda Dehak, Leibny Paola Garcia-Perera, Pedro A. Torres-Carrasquillo, and Najim Dehak, "The JHU-MIT System Description for NIST SRE18", Proc. NIST Speaker Recognition Evaluation Workshop, Athens, Greece, December 2018. [<a href="http://groups.csail.mit.edu/sls/publications/2018/Shon_NIST-2018.pdf" target="_blank">pdf</a>] </td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[2]</td><td><b>Suwon Shon</b>, Najim Dehak, Douglas Reynolds, James Glass, "MCE 2018: The 1st Multi-target speaker detection and identification Challenge Evaluation (MCE) Plan", MCE 2018 plan description [<a href="https://arxiv.org/pdf/1807.06663.pdf" target="_blank">pdf</a>] [<a href="http://mce.csail.mit.edu/" target="_blank">website</a>]</td></tr></tbody>
            
            <tbody><tr><td align="right" valign="top">[1]</td><td><b>Suwon Shon</b> and Hanseok Ko, “KU-ISPL Speaker Recognition Systems under Language Mismatch Condition for NIST 2016 Speaker Recognition Evaluation”,NIST SRE16 workshop, San Diego, USA, December 2016 [<a href="https://arxiv.org/abs/1702.00956" target="_blank">pdf</a>] [<a href="pdf/sre16_poster.pdf" target="_blank">poster</a>]</td></tr></tbody>
            
        </table>
        </td>
        </tr>
    </table>
    

<!-- <h3>System Description (testing..) </h3>
    <p>
        <ul>
            <li> Danwei Cai, Ming Li, "THE DKU-SMIIP SYSTEM FOR MULTI-TARGET SPEAKER DETECTION AND
IDENTIFICATION CHALLENGE EVALUATION 2018" [<a href="http://mce.csail.mit.edu/pdfs/SMIIP-DKU_description.pdf" target="_blank">pdf</a>]</li>
            <li> Kin Wai Cheuk, Balamurali B T, Gemma Roig, Dorien Herremans, "BLACKLISTED SPEAKER IDENTIFICATION USING TRIPLET NEURAL NETWORKS" [<a href="http://mce.csail.mit.edu/pdfs/SUTD_description.pdf" target="_blank">pdf</a>]</li>
            <li> Roberto Font, "BIOMETRIC VOX SYSTEM FOR THE MCE 2018 CHALLENGE" [<a href="http://mce.csail.mit.edu/pdfs/BiometricVox_description.pdf" target="_blank">pdf</a>]</li>
            <li> Qingyu Guo,Xuming Pan, "DESCRIPTION OF SYSTEM FOR MCE 2018" [<a href="http://mce.csail.mit.edu/pdfs/ATS_description.pdf" target="_blank">pdf</a>]</li>
            <li> Nikša Jakovljević, Tijana Delić, Dragiša Mišković, Simona Đurić and Tatjana Lončar Turukalo, "A MULTI-TARGET SPEAKER DETECTION AND IDENTIFICATION SYSTEM
BASED ON COMBINATION OF PLDA AND DNN" [<a href="http://mce.csail.mit.edu/pdfs/vins_description.pdf" target="_blank">pdf</a>]</li>
            <li> Elie Khoury, Khaled Lakhdhar, Andrew Vaughan, Ganesh Sivaraman, Parav Nagarsheth, "PINDROP SUBMISSION TO MCE 2018" [<a href="http://mce.csail.mit.edu/pdfs/Pindrop_description.pdf" target="_blank">pdf</a>]</li>
            <li> Zhi Hao Lim, Wei Rao, Qing Wang, "SYSTEM DESCRIPTION FOR TL@NTU" [<a href="http://mce.csail.mit.edu/pdfs/TL@NTU_description.pdf" target="_blank">pdf</a>]</li>
            <li> Jianbo Ma, Vidhyasaharan Sethu, Eliathamby Ambikairajah, "System Description for MCE 2018" [<a href="http://mce.csail.mit.edu/pdfs/UNSW_EET_description.pdf" target="_blank">pdf</a>]</li>
            <li> SaiKrishna Rallabandi and Alan W Black, "Submission from CMU towards 1st MultiTarget Speaker Detection and
Identification Challenge" [<a href="http://mce.csail.mit.edu/pdfs/TeamCMU_description.pdf" target="_blank">pdf</a>]</li>
            <li> Rohit Saxena, "AREA66 - SYSTEM DESCRIPTION" [<a href="http://mce.csail.mit.edu/pdfs/Area66_description.pdf" target="_blank">pdf</a>]</li>
            <li> Kevin Wilkinghoff, "OPEN-SET SPEAKER RECOGNITION WITH AUGMENTED I-VECTORS" [<a href="http://mce.csail.mit.edu/pdfs/Fraunhofer FKIE_description.pdf" target="_blank">pdf</a>]</li>
            <li> Hongquan Zhao, "THUEE SYSTEM DESCRIPTION FOR MCE2018" [<a href="http://mce.csail.mit.edu/pdfs/THUEE_description.pdf" target="_blank">pdf</a>]</li>

        </ul>
        
    </p> -->
    
    
</td>
</tr>
</tbody></table>
</center>
<br> 
<br> 
<br> 

</div>

</body>


</html>

  
